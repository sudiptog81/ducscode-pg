\documentclass{beamer}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{biblatex} 
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{xcolor}
\usepackage{url}
\usepackage{theorem}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{glossaries}
\usepackage{svg}
\usepackage{parskip}
\usepackage[noend]{algpseudocode}

\usetheme{Madrid}

\AtBeginBibliography{\footnotesize}
\addbibresource{ppt.bib}

\makeatletter
\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor%~~\beamer@ifempty{\insertshortinstitute}{}{(\insertshortinstitute)}
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
    \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
    \insertframenumber{} / \inserttotalframenumber\hspace*{2ex} 
  \end{beamercolorbox}}%
  \vskip0pt%
}

\makeatother

\title{$k$-Medoids Clustering}
\author{Sudipto Ghosh}
\institute{\emph{M.Sc. CS Semester I\\Department of Computer Science\\University of Delhi}}
\date{\today}

\begin{document}
\begin{frame}
    \titlepage
\end{frame}

\begin{frame}
    \frametitle{Motivation}
    \begin{itemize}
        \item In $k$-means clustering, we calculate the arithmetic cluster means and calculate distance from every other point to the cluster means. The cluster mean does not necessarily correspond to a data point.
        \item Can we pick some actual data point as representative elements
        of clusters, and calculate distances from them?
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{$k$-Medoids Clustering}
    \begin{itemize}
        \item Partitioning is performed based on the principle of minimizing the sum of residuals between each data point and its corresponding representative.
        \item Absolute Error Criterion is used
        
        \begin{equation*}
            E = \sum_{i=1}^k \sum_{p \in C_i} dist(p, o_i)
        \end{equation*}
        
        where $E$ is the sum of absolute error for all objects $p$ in the dataset and $o_i$ is the representative point of cluster $C_i$.
    \end{itemize}
\end{frame}

\subsection{Comparison}
\begin{frame}
    \frametitle{Comparison b/w $k$-Means and $k$-Medoids}
    \begin{itemize}
        \item $k$-Medoids method is more robust than $k$-Means in the 
        presence of noise and outliers because a medoid is less influenced by outliers or other extreme values than a mean.
        \item Complexity of each iteration in the $k$-Medoid method is $O(k \cdot (n-k)^2)$. For large databases where $n$ and $k$ are very high, such computations become costlier than $k$-Means.
        \item Both methods require the user to specify $k$ -- the number of clusters.
    \end{itemize}
 \end{frame}

\subsection{PAM}
\begin{frame}
    \frametitle{PAM}
    \begin{itemize}
        \item \textbf Partitioning \textbf Around \textbf Medoids.
        \item Approaches the clustering problem in an iterative, greedy way.
        \item Like the $k$-Means algorithm, the initial representatives are chosen arbitrarily.
        \item Next, we consider whether replacing a representative by a non-representative point would improve the clustering quality.
        \item All possible replacements are carried out. The process continues until  the quality of the resultant clustering cannot be improved by any replacement. Complexity is given by $O(k \cdot (n-k)^2)$.
        \item PAM works well for small databases but not for larger databases. To deal with larger datasets, sampling-based methods can be used.    
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{PAM}
    \begin{enumerate}
        \item Arbitrarily choose $k$-objects in $D$ as initial representatives
        \item Until Convergence
        \begin{enumerate}
            \item Assign each remaining object to the cluster with the nearest representative.
            \item Randomly select a non-representative object $o_{random}$.
            \item Compute the total cost $S$, of swapping representative point $o_j$ with $o_{random}$.
            \item If $S < 0$ then swap $o_j$ with $o_{random}$ to form the new set of $k$-representatives.
        \end{enumerate}
    \end{enumerate}
\end{frame}

\begin{frame}
    \centering \includegraphics[width=350pt]{~/Pictures/Screenshots/Screenshot from 2023-03-12 18-32-36.png}
\end{frame}

\subsection{CLARA}
\begin{frame}
    \frametitle{CLARA}
    \begin{itemize}
        \item \textbf{C}lustering \textbf{LAR}ge \textbf{A}pplications.
        \item Instead of taking the whole dataset into consideration, CLARA uses a random sample of the dataset.
        \item The PAM algorithm is then applied to compute the best medoids from the sample.
        \item CLARA builds clustering from multiple random samples and returns the best clustering as the output. 
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{CLARA}
    \begin{itemize}
        \item Complexity of computing the medoids on a random sample is given by $O(ks^2 + k(n-k))$, where $s$ is the sample size, $k$ is the no. of clusters and $n$ is $|D|$.
        \item Effectiveness of CLARA depends on the sample size.
        \item If an object is one of the best $k$-medoids but is not selected during sampling, CLARA will never find the best clustering.
    \end{itemize}
\end{frame}

\subsection{CLARANS}
\begin{frame}
    \frametitle{CLARANS}
    \begin{itemize}
        \item {\textbf C}lustering {\textbf{L}}arge {\textbf A}pplications based upon {\textbf{RAN}}domized {\textbf S}earch.
        \item Presents a tradeoff betweem the cost and effectiveness of using random samples to obtain clustering.
    \end{itemize}       
\end{frame}

\begin{frame}
    \frametitle{CLARANS}
    \begin{enumerate}
        \item Arbitrarily select $k$ objects in the dataset as the initial medoids.
        \item Randomly select a current medoid $x$ and an object $y$ that is not one of the current medoids.
        \item Replace $x$ by $y$ if it improves the absolute error criterion. Conduct such Randomized Search $l$ times.
        \item The set of current medoids after the $l$ steps is considered a local optimum.
        \item Repeat this randomized process $m$-times and return the best local optimal as the final result.
    \end{enumerate}       
\end{frame}

\subsection{Example - PAM}
\begin{frame}
    \frametitle{PAM - Dry Run}
    \begin{table}[h!]
        \centering
        \begin{tabular}{|c c|} 
             \hline
             x & y \\
             \hline
             8 & 7\\
             3 & 7\\
             4 & 9\\
             9 & 6\\
             8 & 5\\
             5 & 8\\
             7 & 3\\
             8 & 4\\
             7 & 5\\
             4 & 5\\
             \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}
    \frametitle{PAM - Dry Run}
    \centering \includegraphics[width=200pt]{~/Downloads/desmos-graph.png}
\end{frame}

\begin{frame}
    \frametitle{PAM - Dry Run}
    \begin{table}[h!]
        \centering
        \begin{tabular}{|c c|} 
             \hline
             x & y \\
             \hline
             8 & 7\\
             3 & 7\\
             4 & 9\\
             9 & 6\\
             \textbf 8 & \textbf 5\\
             5 & 8\\
             7 & 3\\
             8 & 4\\
             7 & 5\\
             \textbf 4 & \textbf 5\\
             \hline
        \end{tabular}
    \end{table}
\end{frame}

\begin{frame}
    \frametitle{PAM - Dry Run}
    \begin{table}[h!]
        \centering
        \begin{tabular}{|c c c c|} 
             \hline
             x & y & $d(o, c_1)$ & $d(o, c_2)$ \\
             \hline
             8 & 7 & 6 & 2\\
             3 & 7 & 3 & 7\\
             4 & 9 & 4 & 8\\
             9 & 6 & 6 & 2\\
             \textbf 8 & \textbf 5 & - & -\\
             5 & 8 & 4 & 6\\
             7 & 3 & 5 & 3\\
             8 & 4 & 5 & 1\\
             7 & 5 & 3 & 1\\
             \textbf 4 & \textbf 5 & - & -\\
             \hline
        \end{tabular}
    \end{table}
    
    \centering $[(4,5),(3,7),(4,9),(5,8)]$ \\
    \centering $[(8,7),(9,6),(8,5),(7,3),(8,4),(7,5)]$
    
    \centering Cost = (3 + 4 + 4) + (3 + 1 + 1 + 2 + 2) = 20
    
\end{frame}

\begin{frame}
    \frametitle{PAM - Dry Run}
    \begin{table}[h!]
        \centering
        \begin{tabular}{|c c c c|} 
             \hline
             x & y & $d(o, {c'}_1)$ & $d(o, {c'}_2)$ \\
             \hline
             8 & 7 & 6 & 3\\
             3 & 7 & 3 & 8\\
             4 & 9 & 4 & 9\\
             9 & 6 & 6 & 3\\
             8 & 5 & 4 & 1\\
             5 & 8 & 4 & 7\\
             7 & 3 & 5 & 2\\
             \textbf 8 & \textbf 4 & - & -\\
             7 & 5 & 3 & 2\\
             \textbf 4 & \textbf 5 & - & -\\
             \hline
        \end{tabular}
    \end{table}
    
    \centering New Cost = (3 + 4 + 4) + (2 + 2 + 1 + 3 + 3) = 22\\
    \centering New Cost > Cost $\implies$ Undo Swap\\
    \centering $\therefore$ (8, 5) and (4, 5) are the final medoids
\end{frame}

\begin{frame}
    \frametitle{PAM - Dry Run}
    \centering \includegraphics[width=200pt]{~/Downloads/desmos-graph-2.png}
\end{frame}

\begin{frame}
    \frametitle{References}
    \nocite{*}
    \printbibliography 
\end{frame}
\end{document}